<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Siddhant Bansal - EgoProceL</title>
  <meta name="description" content="Siddhant Bansal: EgoProceL">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/egoprocel/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Siddhant Bansal</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
		<li><a href="http://localhost:4000/projects">Projects</a></li>
		<li><a href="http://localhost:4000/experience">Experience</a></li>
		<li><a href="https://sid2697.github.io/Blog_Sid">Blog</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h2 align="center">My View is the Best View:<br />Procedure Learning from Egocentric Videos</h2>

<p style="text-align: center;">
<a href="https://sid2697.github.io" style="color: #CC0000"> Siddhant Bansal</a>
      
<a href="https://www.cse.iitd.ac.in/~chetan/" style="color: #CC0000"> Chetan Arora</a>
      
<a href="https://faculty.iiit.ac.in/~jawahar/index.html" style="color: #CC0000"> C.V. Jawahar</a>
      
</p>
<p style="text-align: center;"><a href="https://eccv2022.ecva.net/" style="color:#CC0000">ECCV 2022</a></p>

<p style="text-align: center;">

<a href="https://sid2697.github.io/egoprocel/" class="btn">Paper</a>
      

<a href="https://sid2697.github.io/egoprocel/#download" class="btn">Dataset</a>
      

<a href="https://github.com/Sid2697/EgoProceL-egocentric-procedure-learning" class="btn">Code</a>
      

</p>

<h3>What is Procedure Learning?</h3>
<div style="text-align: justify">
  <p>Given multiple videos of a task, the goal is to identify the key-steps and their order to perform the task.</p>

  <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/projectpic/procedure-learning.png" width="900px" />
		</div>
    <p>&nbsp;</p>
    <figcaption>
        Provided multiple videos of making a pizza, the goal is to identify the steps required to prepare the pizza and their order.
    </figcaption>
</figure>
</center>

  <h2 align="center"><span style="color:DodgerBlue">EgoProceL Dataset</span></h2>

  <video class="centered" width="100%" autoplay="" muted="" loop="" playsinline="">
  <source src="http://localhost:4000/images/projectpic/EgoProceL-demo.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>
  <figure style="width: 100%; float: left">
    <p class="caption_justify">
    EgoProceL is a large-scale dataset for procedure learning. It consists of 62 hours of egocentric videos recorded by 130 subjects performing 16 tasks for procedure learning. EgoProceL contains videos and key-step annotations for multiple tasks from <a href="http://kitchen.cs.cmu.edu/main.php">CMU-MMAC</a>, <a href="https://cbs.ic.gatech.edu/fpv/">EGTEA Gaze+</a>, and individual tasks like <a href="https://iplab.dmi.unict.it/MECCANO/">toy-bike assembly</a>, <a href="https://sites.google.com/view/epic-tent">tent assembly</a>, PC assembly, and PC disassembly.
  </p>
  </figure>

  <h3> Why an egocentric dataset for Procedure Learning?</h3>
  <p>Using third-person videos for procedure learning makes the manipulated object small in appearance and often occluded by the actor, leading to significant errors.
In contrast, we observe that videos obtained from first-person (egocentric) wearable cameras provide an unobstructed and clear view of the action.</p>

  <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/projectpic/ECCV_diagrams-first_person_vs_third_person_v1.png" width="900px" />
		</div>
</figure>
</center>
  <figure style="width: 100%; float: left">
    <p class="caption_justify">
    Existing datasets majorly consist of third-person videos for procedure learning. 
    Third-person videos contain issues like occlusion and atypical camera locations that makes them ill-suited for procedure learning.
    Additionally, the datasets rely on videos from YouTube that are noisy.
    In contrast, we propose to use egocentric videos that overcome the issues posed by third-person videos.
    Third-person frames in the figure are from ProceL and CrossTask and the first-person frames are from EgoProceL.
  </p>
  </figure>

  <h3> Overview of EgoProceL </h3>

  <p>EgoProceL consists of</p>
  <ul>
    <li><b><u>62</u> hours</b> of videos captured by</li>
    <li><b><u>130</u> subjects</b></li>
    <li>performing <b><u>16</u> tasks</b></li>
    <li>maximum of <b><u>17</u> key-steps</b></li>
    <li>average <b><u>0.38</u> foreground ratio</b></li>
    <li>average <b><u>0.12</u> missing steps ratio</b></li>
    <li>average <b><u>0.49</u> repeated steps ratio</b></li>
  </ul>

  <h3 id="download"><span style="color:DodgerBlue">Downloads</span></h3>
  <p> </p>
  <p>We recommend referring to the <a href="https://github.com/Sid2697/EgoProceL-egocentric-procedure-learning/blob/main/EgoProceL-download-README.md">README</a> before downloading the videos. <a href="http://cvit.iiit.ac.in/research/projects/cvit-projects/egoprocel">Mirror link</a>.</p>

  <h4> Videos </h4>

  <p>Link: <a href="https://iiitaphyd-my.sharepoint.com/:f:/g/personal/siddhant_bansal_research_iiit_ac_in/Ev14SL5JYtJNpVUUAhDMgEABbPnTYpbDUzBYAhQToyHmVw?e=cQu5by">OneDrive</a></p>

  <h4> Annotations </h4>

  <p>Link: <a href="https://iiitaphyd-my.sharepoint.com/:f:/g/personal/siddhant_bansal_research_iiit_ac_in/EgqvXb5syepDv1z-UAwsYEQBivEYauz8tuotty7eey32Ng?e=TNXpBE">OneDrive</a></p>

  <h3 align="center"><span style="color:DodgerBlue">CnC framework for Procedure Learning</span></h3>

  <p>We present a novel self-supervised <b>Correspond and Cut (CnC) framework</b> for procedure learning. CnC identifies and utilizes the temporal correspondences between the key-steps across multiple videos to learn the procedure. Our experiments show that CnC outperforms the state-of-the-art on the benchmark ProceL and CrossTask datasets by 5.2% and 6.3%, respectively.</p>
  <p> </p>
  <center>
<figure>
		<div id="projectid">
    <img src="http://localhost:4000/images/projectpic/ECCV_diagrams-Methodology_v0-5.png" width="900px" />
		</div>
</figure>
</center>
  <p> </p>
  <figure style="width: 100%; float: left">
    <p class="caption_justify">
    CnC takes in multiple videos from the same task and passes them through the embedder network trained using the proposed TC3I loss.
    The goal of the embedder network is to learn similar embeddings for corresponding key-steps from multiple videos and for temporally close frames.
    The ProCut Module (PCM) localizes the key-steps required for performing the task.
    PCM converts the clustering problem to a multi-label graph cut problem.
    The output provides the assignment of frames to the respective key-steps and their ordering.
  </p>
  </figure>

</div>
<p> </p>

<h3> Paper </h3>

<ul>
  <li>arXiv: <b>Coming soon!</b></li>
  <li>ECCV: <b>Coming soon!</b></li>
</ul>

<h3> Code </h3>
<p>The code for this work is available on GitHub!<br />Link: <a href="https://github.com/Sid2697/EgoProceL-egocentric-procedure-learning">Sid2697/EgoProceL-egocentric-procedure-learning</a></p>

<h3> Acknowledgements </h3>

<p style="text-align: justify">
This work was supported in part by the Department of Science and Technology, Government of India, under DST/ICPS/Data-Science project ID T-138. A portion of the data used in this paper was obtained from <a href="http://kitchen.cs.cmu.edu/">kitchen.cs.cmu.edu</a> and the data collection was funded in part by the National Science Foundation under Grant No. EEEC-0540865. We acknowledge <a href="https://scholar.google.com/citations?user=k4TZSPQAAAAJ&amp;hl=en">Pravin Nagar</a> and <a href="https://sagarverma.github.io/">Sagar Verma</a> for recording and sharing the PC Assembly and Disassembly videos at IIIT Delhi. We also acknowledge <a href="https://www.linkedin.com/in/jehlum-pandit/">Jehlum Vitasta Pandit</a> and <a href="https://contemplationanddeepthoughts.home.blog/">Astha Bansal</a> for their help with annotating a portion of EgoProceL.
</p>

<p> </p>

<p>Please consider citing if you make use of the EgoProceL dataset and/or the corresponding code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@InProceedings{EgoProceLECCV2022,
author="Bansal, Siddhant
and Arora, Chetan
and Jawahar, C.V.",
title="My View is the Best View: Procedure Learning from Egocentric Videos",
booktitle = "European Conference on Computer Vision (ECCV)",
year="2022"
}
</code></pre></div></div>

<p> </p>
<p> </p>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Siddhant Bansal. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
