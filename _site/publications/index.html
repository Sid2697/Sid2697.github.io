<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Siddhant Bansal - Publications</title>
  <meta name="description" content="Siddhant Bansal -- Publications.">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/publications/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Siddhant Bansal</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
		<li><a href="http://localhost:4000/projects">Projects</a></li>
		<li><a href="http://localhost:4000/experience">Experience</a></li>
		<li><a href="https://sid2697.github.io/Blog_Sid">Blog</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <h3 id="publicationsreports">Publications/Reports</h3>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/hoi-ref.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>We propose the HOI-Ref task to understand hand-object using Vision Language Models (VLMs). We introduce the HOI-QA dataset with 3.9M question-answer pairs for training and evaluating VLMs. Finally, we train the first VLM for HOI-Ref, achieving state-of-the-art performance.</p>
    </div>

    <div style="text-align: justify">
      <p><em><b>Siddhant Bansal</b>, <a href="https://mwray.github.io/">Michael Wray</a>, <a href="https://dimadamen.github.io/">Dima Damen</a></em></p>
    </div>

    <p></p>

    <p><a href="">Paper (coming soon)</a>
 /
 <a href="https://sid2697.github.io/hoi-ref/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/HOI-Ref">Code</a>
 /
 <a href="">Gradio Demo (coming soon)</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/ego-exo4d.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>Ego-Exo4D is a diverse, large-scale multi-modal, multi-view, video dataset and benchmark collected across 13 cities worldwide by 839 camera wearers, capturing 1422 hours of video of skilled human activities.<br />We present three synchronized natural language datasets paired with videos. (1) expert commentary, (2) participant-provided narrate-and-act, and (3) one-sentence atomic action descriptions. <br />Finally, our camera configuration features Aria glasses for ego capture which is time-synchronized with 4-5 (stationary) GoPros as the exo capture devices.</p>
    </div>

    <div style="text-align: justify">
      <p><em>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, <a href="https://sid2697.github.io/"><b>Siddhant Bansal</b></a>, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray</em></p>
    </div>

    <p>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024</p>

    <p><a href="https://arxiv.org/abs/2311.18259">Paper</a>
 /
 <a href="https://ego-exo4d-data.org">Project Page</a>
 /
 <a href="https://youtu.be/GdooXEBAnI8">Video</a>
 /
 <a href="https://ai.meta.com/blog/ego-exo4d-video-learning-perception/">Meta blog post</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/unity_graph.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>We propose Graph-based Procedure Learning (GPL) framework for procedure learning. GPL creates novel UnityGraph that represents <i>all</i> the task videos as a graph to encode both intra-video and inter-videos context. We achieve an improvement of 2% on third-person datasets and 3.6% on EgoProceL.</p>
    </div>

    <div style="text-align: justify">
      <p><em><b>Siddhant Bansal</b>, <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a></em></p>
    </div>

    <p>Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2024</p>

    <p><a href="https://arxiv.org/pdf/2311.03550">Paper</a>
 /
 <a href="https://sid2697.github.io/egoprocel/#download">Download the EgoProceL Dataset</a>
 /
 <a href="https://sid2697.github.io/unitygraph/">Project Page</a>
 /
 <a href="./../docs/wacv24-236.mp4">Video</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>An Outlook into the Future of Egocentric Vision</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/survey_image.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>The survey looks at the difference between what we’re studying now in egocentric vision and what we expect in the future. We imagine the future using stories and connect them to current research. We highlight problems, analyze current progress, and suggest areas to explore in egocentric vision, aiming for a future that’s always on, personalized, and improves our lives.</p>
    </div>

    <div style="text-align: justify">
      <p><em><a href="https://chiaraplizz.github.io">Chiara Plizzari*</a>, <a href="https://ezius07.github.io">Gabriele Goletto*</a>, <a href="https://www.antoninofurnari.it">Antonino Furnari*</a>, <b>Siddhant Bansal*</b>, <a href="https://iplab.dmi.unict.it/ragusa/">Francesco Ragusa*</a>, <a href="https://www.dmi.unict.it/farinella/">Giovanni Maria Farinella</a>, <a href="https://dimadamen.github.io">Dima Damen</a>, <a href="http://www.tatianatommasi.com">Tatiana Tommasi</a></em></p>
    </div>

    <p><i>Under Review</i></p>

    <p><a href="https://openreview.net/forum?id=V3974SUk1w">Paper with comments (OpenReview)</a>
 /
 <a href="https://arxiv.org/abs/2308.07123">Paper (arXiv)</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>My View is the Best View: Procedure Learning from Egocentric Videos</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/CVPR_diagrams-Correspondences_v1_2.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>We propose the <b>EgoProceL dataset</b> consisting of 62 hours of videos captured by 130 subjects performing 16 tasks and a self-supervised <b>Correspond and Cut (CnC) framework</b> for procedure learning. CnC utilizes the temporal correspondences between the key-steps across multiple videos to learn the procedure.</p>
    </div>

    <div style="text-align: justify">
      <p><em><b>Siddhant Bansal</b>, <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a></em></p>
    </div>

    <p>European Conference on Computer Vision (<b>ECCV</b>), 2022</p>

    <p><a href="https://arxiv.org/pdf/2207.10883">Paper</a>
 /
 <a href="https://sid2697.github.io/egoprocel/#download"><b>Download the EgoProceL Dataset</b></a>
 /
 <a href="https://sid2697.github.io/egoprocel/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/EgoProceL-egocentric-procedure-learning">Code</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>Ego4D: Around the World in 3,000 Hours of Egocentric Video</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/ego4d.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>We offer 3,670 hours of daily-life activity video spanning hundreds of scenarios captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries.<br />We present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities).</p>
    </div>

    <div style="text-align: justify">
      <p><em><br />Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, <a href="https://sid2697.github.io/"><b>Siddhant Bansal</b></a>, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik</em></p>
    </div>

    <p>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <span style="color:red;">(ORAL; Best paper finalist [<a href="https://twitter.com/CVPR/status/1539258447752994816">link</a>])</span></p>

    <p><a href="https://arxiv.org/abs/2110.07058">Paper</a>
 /
 <a href="https://ego4d-data.org/">Project Page</a>
 /
 <a href="https://www.youtube.com/watch?v=taC2ZKl9IsE">Video</a>
 /
 <a href="https://drive.google.com/file/d/1oknfQIH9w1rXy6I1j5eUE6Cqh96UwZ4L/view">Benchmark’s description</a>
 /
 <a href="https://eyewear-computing.org/EPIC_ICCV21/">EPIC@ICCV2021 Ego4D Reveal Session</a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>Improving Word Recognition using Multiple Hypotheses and Deep Embeddings</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/Confidence_module.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>We propose to fuse recognition-based and recognition-free approaches for word recognition using learning-based methods.</p>
    </div>

    <div style="text-align: justify">
      <p><em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>
    </div>

    <p>International Conference on Pattern Recognition (<b>ICPR</b>), 2020</p>

    <p><a href="https://arxiv.org/pdf/2010.14411.pdf">Paper</a>
 /
 <a href="https://sid2697.github.io/embednet_cab/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/Word-recognition-EmbedNet-CAB">Code (GitHub)</a>
 /
 <a href="https://youtu.be/T_TYL-_HpbY">Video (YouTube)<br /><br /></a></p>

  </div>
</div>

<div class="col-sm-11 clearfix">
  <div class="well">
    <pubtit>Fused Text Recogniser and Deep  Embeddings Improve  Word  Recognition  and  Retrieval</pubtit>

    <p><img src="http://localhost:4000/images/pubpic/WordRecVisualisation_website.png" class="img-responsive" width="200px" style="float: left" /></p>

    <div style="text-align: justify">
      <p>Fusing recognition-based and recognition-free approaches using rule-based methods for improving word recognition and retrieval.</p>
    </div>

    <div style="text-align: justify">
      <p><em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>
    </div>

    <p>IAPR International Workshop on Document Analysis and System (<b>DAS</b>), 2020 <span style="color:red;">(ORAL)</span></p>

    <p><a href="https://arxiv.org/pdf/2007.00166.pdf">Paper</a>
 /
 <a href="../images/pubpic/Word_Retrieval_demo.gif">Demo</a>
 /
 <a href="https://sid2697.github.io/Word-recognition-and-retrieval/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/Word-recognition-and-retrieval">Code (Github)</a>
 /
 <a href="../papers/Siddhant_Bansal_V4.pdf">Poster</a></p>

  </div>
</div>

<p>   </p>

<!-- ### Theses



<div class="col-sm-11 clearfix">
 <div class="well">
 <pubtit>Learning Inference Models for Computer Vision</pubtit>

 <img src="http://localhost:4000/images/pubpic/phd_thesis_teaser.png" class="img-responsive" width="200px" style="float: left" />

 <p>Learning based techniques for better inference in several computer vision models ranging from inverse graphics to freely parameterized neural networks.</p>

 <p><em>V. Jampani</em></p>

 <p>PhD Thesis, MPI for Intelligent Systems and University of T&uuml;bingen, December, 2016</p>

 

 

 
 <p><a href="https://varunjampani.github.io/papers/jampani16_phd_thesis.pdf">pdf</a>
 /
 <a href="https://varunjampani.github.io/papers/phd_thesis_slides.pdf">slides</a>
 /
 <a href="https://publikationen.uni-tuebingen.de/xmlui/handle/10900/76486">library</a></p>
 

 

 

 </div>
</div>



<div class="col-sm-11 clearfix">
 <div class="well">
 <pubtit>A Study of X-Ray Image Perception for Pneumoconiosis Detection</pubtit>

 <img src="http://localhost:4000/images/pubpic/ms_thesis_teaser.png" class="img-responsive" width="200px" style="float: left" />

 <p>Eye tracking experimental studies and models for X-Ray Image Perception and Diagnosis.</p>

 <p><em>V. Jampani</em></p>

 <p>Master Thesis, IIIT-Hyderabad, January, 2013</p>

 
 <p><a href="http://files.is.tue.mpg.de/vjampani/jampani13_MSTHESIS.pdf">pdf</a></p>
 

 

 

 

 

 </div>
</div>



<p> &nbsp; </p> -->

<!-- ## Full List



  HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision <br />
  <em><b>Siddhant Bansal</b>, <a href="https://mwray.github.io/">Michael Wray</a>, <a href="https://dimadamen.github.io/">Dima Damen</a> </em><br /><a href=""></a>



  Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives <br />
  <em>Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, <a href='https://sid2697.github.io/'><b>Siddhant Bansal</b></a>, Bikram Boote, Eugene Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria Escobar, Cristhian Forigua, Abrham Gebreselasie, Sanjay Haresh, Jing Huang, Md Mohaiminul Islam, Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao, Miguel Martin, Effrosyni Mavroudi, Tushar Nagarajan, Francesco Ragusa, Santhosh Kumar Ramakrishnan, Luigi Seminara, Arjun Somayazulu, Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu Zhang, Angela Castillo, Changan Chen, Xinzhu Fu, Ryosuke Furuta, Cristina Gonzalez, Prince Gupta, Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo, Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu, Mi Luo, Zhengyi Luo, Brighid Meredith, Austin Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny Peng, Shraman Pramanick, Merey Ramazanova, Fiona Ryan, Wei Shan, Kiran Somasundaram, Chenan Song, Audrey Southerland, Masatoshi Tateno, Huiyu Wang, Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang, Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas Bertasius, David Crandall, Dima Damen, Jakob Engel, Giovanni Maria Farinella, Antonino Furnari, Bernard Ghanem, Judy Hoffman, C. V. Jawahar, Richard Newcombe, Hyun Soo Park, James M. Rehg, Yoichi Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou, Michael Wray </em><br /><a href=""></a>



  United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos <br />
  <em><b>Siddhant Bansal</b>, <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a> </em><br /><a href=""></a>



  An Outlook into the Future of Egocentric Vision <br />
  <em><a href='https://chiaraplizz.github.io'>Chiara Plizzari*</a>, <a href='https://ezius07.github.io'>Gabriele Goletto*</a>, <a href='https://www.antoninofurnari.it'>Antonino Furnari*</a>, <b>Siddhant Bansal*</b>, <a href='https://iplab.dmi.unict.it/ragusa/'>Francesco Ragusa*</a>, <a href='https://www.dmi.unict.it/farinella/'>Giovanni Maria Farinella</a>, <a href='https://dimadamen.github.io'>Dima Damen</a>, <a href='http://www.tatianatommasi.com'>Tatiana Tommasi</a> </em><br /><a href=""></a>



  My View is the Best View: Procedure Learning from Egocentric Videos <br />
  <em><b>Siddhant Bansal</b>, <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a> </em><br /><a href=""></a>



  Ego4D: Around the World in 3,000 Hours of Egocentric Video <br />
  <em><br>Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, <a href='https://sid2697.github.io/'><b>Siddhant Bansal</b></a>, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik </em><br /><a href=""></a>



  Improving Word Recognition using Multiple Hypotheses and Deep Embeddings <br />
  <em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a> </em><br /><a href=""></a>



  Fused Text Recogniser and Deep  Embeddings Improve  Word  Recognition  and  Retrieval <br />
  <em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a> </em><br /><a href=""></a>

 -->

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Siddhant Bansal. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
