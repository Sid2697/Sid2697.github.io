<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Siddhant Bansal - Home</title>
  <meta name="description" content="Siddhant Bansal">
  <link rel="stylesheet" href="http://localhost:4000/css/main.css">
  <link rel="canonical" href="http://localhost:4000/">
<link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000/images/favicon.ico">



</head>


  <body>

    <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container-fluid">
	<div class="navbar-header">
	  <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar-collapse-1" aria-expanded="false">
		<span class="sr-only">Toggle navigation</span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	  </button>

    <a class="navbar-brand" href="http://localhost:4000/">Siddhant Bansal</a>
	</div>
	<div class="collapse navbar-collapse" id="navbar-collapse-1">
	  <ul class="nav navbar-nav navbar-right">
		<li><a href="http://localhost:4000/">Home</a></li>
		<li><a href="http://localhost:4000/publications">Publications</a></li>
		<li><a href="http://localhost:4000/projects">Projects</a></li>
		<li><a href="http://localhost:4000/experience">Experience</a></li>
		<li><a href="https://sid2697.github.io/Blog_Sid">Blog</a></li>
	  </ul>
	</div>
  </div>
</div>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12">
  <div class="container-fluid">

  <div class="row">

    <div class="col-sm-8">

      <div style="text-align: justify">
        <p>I am a first-year PhD student at the <a href="https://uob-mavi.github.io/people/">University of Bristol</a> working with <a href="https://dimadamen.github.io">Prof. Dima Damen</a>.
My research interest is Computer Vision, Pattern Recognition, and Machine Learning. 
Currently, I am working on devising learning-based methods for understanding and exploring various aspects of first-person (egocentric) vision.
Previously, at <a href="http://cvit.iiit.ac.in">CVIT, IIIT Hyderabad</a>, I worked with <a href="https://faculty.iiit.ac.in/~jawahar/index.html">Prof. C.V. Jawahar</a> and <a href="https://www.cse.iitd.ac.in/~chetan/">Prof. Chetan Arora</a> on <a href="https://sid2697.github.io/egoprocel/">unsupervised procedure learning</a> from egocentric videos.
<!-- I am an MS by Research candidate at <a href="http://cvit.iiit.ac.in">CVIT, IIIT Hyderabad</a>. I work with <a href="https://faculty.iiit.ac.in/~jawahar/index.html">Prof. C.V. Jawahar</a> and <a href='https://www.cse.iitd.ac.in/~chetan/'>Prof. Chetan Arora</a>. -->
Earlier, I worked on improving word recognition and retrieval in large document collection with <a href="https://faculty.iiit.ac.in/~jawahar/index.html">Prof. C.V. Jawahar</a> and on 3D Computer Vision with <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a>.</p>
      </div>

      <div style="text-align: justify">
        <p>My ultimate goal is to contribute to the development of systems capable of understanding the world as we do. I’m an inquisitive person, and I’m always willing to learn about fields including, but not limited to, science, technology, astrophysics, and physics.</p>
      </div>

      <p align="center">
  <a href="./docs/Siddhant_Bansal.pdf">CV</a> /
  <a href="https://scholar.google.com/citations?hl=en&amp;user=ciok5VwAAAAJ">Google Scholar</a> /
  <a href="https://github.com/Sid2697">Github</a> /
  <a href="https://www.linkedin.com/in/siddhant-bansal/">LinkedIn</a> /
  <a href="https://arxiv.org/a/bansal_s_1.html"> arXiv </a> /
  <a href="https://orcid.org/0000-0003-2636-0066">ORCID</a>
</p>

      <h3 id="news">News</h3>
      <hr />

      <p>Oct, 2023 :
<em>United We Stand, Divided We Fall: UnityGraph for Unsupervised Procedure Learning from Videos got accepted to <a href="https://wacv2024.thecvf.com">WACV 2024</a>!</em></p>

      <p>Aug, 2023 :
<em>Survey paper “<a href="https://openreview.net/forum?id=V3974SUk1w">An Outlook into the Future of Egocentric Vision</a>” open for comments on OpenReview until 15 Sep. All major corrections/suggestions will be acknowledged in revised version.</em></p>

      <p>June, 2023 :
<em>Co-organising the <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/home">Joint International 3rd Ego4D and 11th EPIC Workshop @ CVPR 2023</a>.</em></p>

      <p>April, 2023 :
<em>Joining <a href="https://uob-mavi.github.io/people/">University of Bristol</a> as a PhD student. I will be working with <a href="https://dimadamen.github.io">Prof. Dima Damen</a> on egocentric video understanding.</em></p>

      <p>Feb, 2023 :
<em>Successfully defended my master’s thesis, <b>I-Do, You-Learn: Techniques for Unsupervised Procedure Learning using Egocentric Videos</b>.</em></p>

      <p>Dec, 2022 :
<em>Gave a talk on <a href="https://iiitaphyd-my.sharepoint.com/:p:/g/personal/siddhant_bansal_research_iiit_ac_in/EbxapAwL-hxCjJyEMYa8aXgBHeKsYTbhqonfQzT0hUIltg?e=W9z8Ki">Procedure Learning from Egocentric Videos</a> at <a href="https://events.iitgn.ac.in/2022/icvgip/index.html">ICVGIP 2022</a> <a href="https://events.iitgn.ac.in/2022/icvgip/vision_india.html">(Vision India)</a>, hosted by <a href="https://people.iitgn.ac.in/~shanmuga/">Prof. Shanmuganathan Raman</a> and <a href="http://home.iitj.ac.in/~rn/">Dr. Rajendra Nagar</a>.</em></p>

      <p>Nov, 2022 :
<em>Gave a talk on <a href="https://iiitaphyd-my.sharepoint.com/:p:/g/personal/siddhant_bansal_research_iiit_ac_in/EX5YUA772apKte6EsNcZX4IBwVh5Xz4cmWGvDAvXmYzm2w?e=KvoTKi">Procedure Learning from Egocentric Videos</a> at <a href="http://www.cvc.uab.es/">Computer Vision Centre, Universitat Autònoma de Barcelona</a>, hosted by <a href="https://scholar.google.com/citations?user=xASEtrUAAAAJ&amp;hl=en">Prof. Dimosthenis Karatzas</a>.</em></p>

      <h4 id="see-all-news"><a href="http://localhost:4000/allnews.html">See all news</a></h4>

    </div>

    <div class="col-sm-4" style="display:table-cell; vertical-align:left; text-align:left">

      <ul style="overflow: hidden">
  <img src="http://localhost:4000/images/profile_pic.jpeg" class="img-responsive" width="100%" />
  </ul>

      <p><!-- <br clear="all" /> --></p>
      <div style="text-align: right">
        <p>Machine Learning and Computer Vision (<b>MaVi</b>) @
  University of Bristol</p>
      </div>

    </div>

  </div>
</div>

<div class="col-sm-12">

  <h3 id="publications">Publications</h3>
  <hr />

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>An Outlook into the Future of Egocentric Vision</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/survey_image.png" class="img-responsive" width="250px" style="float: left" /></p>

      <div style="text-align: justify">
        <p>The survey looks at the difference between what we’re studying now in egocentric vision and what we expect in the future. We imagine the future using stories and connect them to current research. We highlight problems, analyze current progress, and suggest areas to explore in egocentric vision, aiming for a future that’s always on, personalized, and improves our lives.</p>
      </div>

      <div style="text-align: justify">
        <p><em><a href="https://chiaraplizz.github.io">Chiara Plizzari*</a>, <a href="https://ezius07.github.io">Gabriele Goletto*</a>, <a href="https://www.antoninofurnari.it">Antonino Furnari*</a>, <b>Siddhant Bansal*</b>, <a href="https://iplab.dmi.unict.it/ragusa/">Francesco Ragusa*</a>, <a href="https://www.dmi.unict.it/farinella/">Giovanni Maria Farinella</a>, <a href="https://dimadamen.github.io">Dima Damen</a>, <a href="http://www.tatianatommasi.com">Tatiana Tommasi</a></em></p>
      </div>

      <p><i>Under Review</i></p>

      <p><a href="https://openreview.net/forum?id=V3974SUk1w">Paper with comments (OpenReview)</a>
 /
 <a href="https://arxiv.org/abs/2308.07123">Paper (arXiv)</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>My View is the Best View: Procedure Learning from Egocentric Videos</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/CVPR_diagrams-Correspondences_v1_2.png" class="img-responsive" width="250px" style="float: left" /></p>

      <div style="text-align: justify">
        <p>We propose the <b>EgoProceL dataset</b> consisting of 62 hours of videos captured by 130 subjects performing 16 tasks and a self-supervised <b>Correspond and Cut (CnC) framework</b> for procedure learning. CnC utilizes the temporal correspondences between the key-steps across multiple videos to learn the procedure.</p>
      </div>

      <div style="text-align: justify">
        <p><em><b>Siddhant Bansal</b>, <a href="https://www.cse.iitd.ac.in/~chetan/">Chetan Arora</a>, <a href="https://faculty.iiit.ac.in/~jawahar/index.html">C.V. Jawahar</a></em></p>
      </div>

      <p>European Conference on Computer Vision (<b>ECCV</b>), 2022</p>

      <p><a href="https://arxiv.org/pdf/2207.10883">Paper</a>
 /
 <a href="https://sid2697.github.io/egoprocel/#download"><b>Download the EgoProceL Dataset</b></a>
 /
 <a href="https://sid2697.github.io/egoprocel/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/EgoProceL-egocentric-procedure-learning">Code</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Ego4D: Around the World in 3,000 Hours of Egocentric Video</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/ego4d.png" class="img-responsive" width="250px" style="float: left" /></p>

      <div style="text-align: justify">
        <p>We offer 3,670 hours of daily-life activity video spanning hundreds of scenarios captured by 931 unique camera wearers from 74 worldwide locations and 9 different countries.<br />We present a host of new benchmark challenges centered around understanding the first-person visual experience in the past (querying an episodic memory), present (analyzing hand-object manipulation, audio-visual conversation, and social interactions), and future (forecasting activities).</p>
      </div>

      <div style="text-align: justify">
        <p><em><br />Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, <a href="https://sid2697.github.io/"><b>Siddhant Bansal</b></a>, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, Jitendra Malik</em></p>
      </div>

      <p>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2022 <span style="color:red;">(ORAL; Best paper finalist [<a href="https://twitter.com/CVPR/status/1539258447752994816">link</a>])</span></p>

      <p><a href="https://arxiv.org/abs/2110.07058">Paper</a>
 /
 <a href="https://ego4d-data.org/">Project Page</a>
 /
 <a href="https://www.youtube.com/watch?v=taC2ZKl9IsE">Video</a>
 /
 <a href="https://drive.google.com/file/d/1oknfQIH9w1rXy6I1j5eUE6Cqh96UwZ4L/view">Benchmark’s description</a>
 /
 <a href="https://eyewear-computing.org/EPIC_ICCV21/">EPIC@ICCV2021 Ego4D Reveal Session</a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Improving Word Recognition using Multiple Hypotheses and Deep Embeddings</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/Confidence_module.png" class="img-responsive" width="250px" style="float: left" /></p>

      <div style="text-align: justify">
        <p>We propose to fuse recognition-based and recognition-free approaches for word recognition using learning-based methods.</p>
      </div>

      <div style="text-align: justify">
        <p><em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>
      </div>

      <p>International Conference on Pattern Recognition (<b>ICPR</b>), 2020</p>

      <p><a href="https://arxiv.org/pdf/2010.14411.pdf">Paper</a>
 /
 <a href="https://sid2697.github.io/embednet_cab/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/Word-recognition-EmbedNet-CAB">Code (GitHub)</a>
 /
 <a href="https://youtu.be/T_TYL-_HpbY">Video (YouTube)<br /><br /></a></p>

    </div>
  </div>

  <div class="col-sm-11 clearfix">
    <div class="well">
      <pubtit>Fused Text Recogniser and Deep  Embeddings Improve  Word  Recognition  and  Retrieval</pubtit>

      <p><img src="http://localhost:4000/images/pubpic/WordRecVisualisation_website.png" class="img-responsive" width="250px" style="float: left" /></p>

      <div style="text-align: justify">
        <p>Fusing recognition-based and recognition-free approaches using rule-based methods for improving word recognition and retrieval.</p>
      </div>

      <div style="text-align: justify">
        <p><em><b>Siddhant Bansal</b>, <a href="https://kris314.github.io"> Praveen Krishnan </a>, and <a href="https://faculty.iiit.ac.in/~jawahar/index.html"> C.V. Jawahar </a></em></p>
      </div>

      <p>IAPR International Workshop on Document Analysis and System (<b>DAS</b>), 2020 <span style="color:red;">(ORAL)</span></p>

      <p><a href="https://arxiv.org/pdf/2007.00166.pdf">Paper</a>
 /
 <a href="../images/pubpic/Word_Retrieval_demo.gif">Demo</a>
 /
 <a href="https://sid2697.github.io/Word-recognition-and-retrieval/">Project Page</a>
 /
 <a href="https://github.com/Sid2697/Word-recognition-and-retrieval">Code (Github)</a>
 /
 <a href="../papers/Siddhant_Bansal_V4.pdf">Poster</a></p>

    </div>
  </div>

  <p><br clear="all" /></p>

  <h4 id="see-all-publications"><a href="http://localhost:4000/publications">See all publications</a></h4>

</div>

<div class="col-sm-12">

  <h3 id="miscellaneous">Miscellaneous</h3>
  <hr />
  <h4 id="invited-talks">Invited Talks</h4>

  <ul>
    <li>
      <p>Egocentric Videos for Procedure Learning @ <a href="https://events.iitgn.ac.in/2022/icvgip/index.html">Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2022)</a> (<a href="https://events.iitgn.ac.in/2022/icvgip/vision_india.html">Vision India</a>). [<a href="https://iiitaphyd-my.sharepoint.com/:p:/g/personal/siddhant_bansal_research_iiit_ac_in/EbxapAwL-hxCjJyEMYa8aXgBHeKsYTbhqonfQzT0hUIltg?e=W9z8Ki">slides</a>; <a href="https://twitter.com/Sid__Bansal/status/1602165700424273920?s=20&amp;t=BOQDMb1dCMppgcCjHaA9KA">tweet</a>; <a href="https://www.linkedin.com/posts/siddhant-bansal_icvgip2022-eccv2022-activity-7007934958484746240-wBva?utm_source=share&amp;utm_medium=member_desktop">linkedin</a>]</p>
    </li>
    <li>
      <p>Egocentric Videos for Procedure Learning @ <a href="https://iplab.dmi.unict.it/fpv/">IPLAB, University of Catania</a> [<a href="https://iiitaphyd-my.sharepoint.com/:p:/g/personal/siddhant_bansal_research_iiit_ac_in/ERc_foZHgKZEnG9xhQKvXS8BWeQOgosXJCpnhM1YBad98Q?e=C4eyS4">slides</a>; <a href="https://twitter.com/Sid__Bansal/status/1588149526459736064?s=20&amp;t=CTjlbGTkjcmHUS8tbbJ0Qw">tweet</a>]</p>
    </li>
    <li>
      <p>Egocentric Videos for Procedure Learning @ <a href="http://www.cvc.uab.es">Computer Vision Centre, Universitat Autònoma de Barcelona</a> [<a href="https://iiitaphyd-my.sharepoint.com/:p:/g/personal/siddhant_bansal_research_iiit_ac_in/EX5YUA772apKte6EsNcZX4IBwVh5Xz4cmWGvDAvXmYzm2w?e=KvoTKi">slides</a>; <a href="https://twitter.com/dkaratzas/status/1590723083198873604?s=20&amp;t=EAKrnmTUI0IsHTh5mQ98Tw">tweet</a>]</p>
    </li>
  </ul>

  <h4 id="workshop-organizer">Workshop Organizer</h4>

  <ul>
    <li><a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/home">Joint International 3rd Ego4D and 11th EPIC Workshop @ CVPR 2023</a></li>
    <li><a href="https://ego4d-data.org/workshops/eccv22/">2nd International Ego4D Workshop @ ECCV 2022</a></li>
  </ul>

  <h4 id="conference-reviewer">Conference Reviewer</h4>

  <ul>
    <li>CVPR 2022, 2023</li>
    <li>ICCV 2023</li>
    <li>ECCV 2022</li>
    <li>WACV 2023, 2024</li>
  </ul>

  <h4 id="journal-reviewer">Journal Reviewer</h4>

  <ul>
    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">T-PAMI</a>)</li>
    <li>Computer Vision and Image Understanding (<a href="https://www.sciencedirect.com/journal/computer-vision-and-image-understanding">CVIU</a>)</li>
  </ul>

  <h4 id="workshop-reviewer">Workshop Reviewer</h4>

  <ul>
    <li><a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">Joint 1st Ego4D and 10th EPIC Workshop @ CVPR 2022</a></li>
  </ul>

  <p>   </p>

  <h3 id="blog">Blog</h3>
  <hr />

  <h3 id="deep-future-gaze-gaze-anticipation-on-egocentric-videos-using-adversarial-networks">Deep Future Gaze: Gaze Anticipation on Egocentric Videos Using Adversarial Networks</h3>
  <p>This paper proposes a <em>Generative Adversarial Network</em> (GAN) based architecture called Deep Future Gaze (DFG) for addressing the task of <em>gaze anticipation</em> in egocentric videos.</p>
  <h4 id="link-to-the-article"><a href="https://sid2697.github.io/Blog_Sid/paper_summary/2020/09/01/Deep-future-gaze.html">Link</a> to the article!</h4>

  <h3 id="first-person-action-recognition-using-deep-learned-descriptors">First Person Action Recognition Using Deep Learned Descriptors</h3>
  <p>This paper proposes a three-stream convolutional neural network architecture for the task of action recognition in first-person videos.</p>
  <h4 id="link-to-the-article-1"><a href="https://sid2697.github.io/Blog_Sid/paper_summary/2020/08/19/action-descriptors-Suriya.html">Link</a> to the article!</h4>

  <h3 id="two-stream-convolutional-networks-for-action-recognition-in-videos">Two-Stream Convolutional Networks for Action Recognition in Videos</h3>
  <p>This paper proposes a two-stream convolutional neural network architecture for the task of action recognition in a video.</p>
  <h4 id="link-to-the-article-2"><a href="https://sid2697.github.io/Blog_Sid/paper_summary/2020/08/16/Two-Stream-Zisserman-nips2014.html">Link</a> to the article!</h4>

  <p>   </p>
  <h4 id="see-all-articles"><a href="https://sid2697.github.io/Blog_Sid">See all articles</a></h4>

  <p>   </p>

</div>

</div>

      </div>
    </div>

    <div id="footer" class="panel">
  <div class="panel-footer">
	<div class="container-fluid">
	  <div class="row">
		<div class="col-sm-5">

		  <p>&copy 2022 Siddhant Bansal. Site made with <a href="https://jekyllrb.com">Jekyll</a>.</p>
		   <p>  </p><p>


		</div>
		<div class="col-sm-5">
		</div>
    <div class="col-sm-5">
		</div>
		<div class="col-sm-5">
		</div>
	  </div>
	</div>
  </div>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
<script src="http://localhost:4000/js/bootstrap.min.js"></script>


  </body>

</html>
