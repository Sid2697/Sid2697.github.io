---
title: "Siddhant Bansal - Word Recognition"
layout: gridlay
excerpt: "Siddhant Bansal: Word Recognition"
sitemap: false
permalink: /embednet_cab/
---

[comment]: Title
<h2 align="center"> Improving Word Recognition using Multiple Hypotheses and Deep Embeddings</h2>
<p>&nbsp;</p>

[comment]: Authors
<p style="text-align: center;">
<a href="https://sid2697.github.io" style="color: #CC0000"> Siddhant Bansal </a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://kris314.github.io/" style="color: #CC0000"> Praveen Krishnan </a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://faculty.iiit.ac.in/~jawahar/index.html" style="color: #CC0000"> C.V. Jawahar </a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</p>
<p>&nbsp;</p>

[comment]: Abstract
<h3> Abstract </h3>
We propose to fuse recognition-based and recognition-free approaches for word recognition using learning-based methods. 
For this purpose, results obtained using a text recognizer and deep embeddings (generated using an End2End network) are fused.
To further improve the embeddings, we propose EmbedNet, it uses triplet loss for training and learns an embedding space where the embedding of the word image lies closer to its corresponding text transcription's embedding.
This updated embedding space helps in choosing the correct prediction with higher confidence.
To further improve the accuracy, we propose a plug-and-play module called Confidence based Accuracy Booster (CAB). 
It takes in the confidence scores obtained from the text recognizer and Euclidean distances between the embeddings and generates an updated distance vector.
This vector has lower distance values for the correct words and higher distance values for the incorrect words.
We rigorously evaluate our proposed method systematically on a collection of books that are in the Hindi language.
Our method achieves an absolute improvement of around 10% in terms of word recognition accuracy.

<center>
<figure>
		<div id="projectid">
    <img src="{{ site.url }}{{ site.baseurl }}/images/projectpic/EmbedNet_ProcessFlow.pdf" width="900px" />
		</div>
		<br />
    <figcaption>
        For generating the textual transcription, we pass the word image through the CRNN and the End2End network (E2E), simultaneously.
        The CRNN generates multiple (K) textual transcriptions for the input image, whereas the E2E network generates the word image's embedding.
        The K textual transcriptions generated by the CRNN are passed through the E2E network to generate their embeddings.
        We pass these embeddings through the EmbedNet proposed in this work.
        The EmbedNet projects the input embedding to an updated Euclidean space, using which we get updated word image embedding and K transcriptions' embedding.
        We calculate the Euclidean distance between the input embedding and each of the K textual transcriptions.
        We then pass the distance values through the novel Confidence based Accuracy Booster (CAB), which uses them and the confidence scores from the CRNN to generate an updated list of Euclidean distance, which helps in selecting the correct prediction.
    </figcaption>
</figure>
</center>

[comment]: Paper
<h3> Paper </h3>
<b>Coming soon</b>!

<!-- - Paper: <a href="{{ site.url }}{{ site.baseurl }}/papers/hung19_SCOPS.pdf" style="color: #CC0000"> PDF </a>
- Supplementary: <a href="{{ site.url }}{{ site.baseurl }}/papers/hung19_SCOPS_supp.pdf" style="color: #CC0000"> PDF </a> -->

Please consider citing if you make use of this work and/or the corresponding code:
<b>Coming soon</b>!

<!-- ```
@inproceedings{hung:CVPR:2019,
	title = {SCOPS: Self-Supervised Co-Part Segmentation},
	author = {Hung, Wei-Chih and Jampani, Varun and Liu, Sifei and Molchanov, Pavlo and Yang, Ming-Hsuan and Kautz, Jan},
	booktitle = {IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)},
	month = june,
	year = {2019}
}
``` -->

[comment]: Code
<h3> Code </h3>
This work is implemented using <a href="https://pytorch.org/" style="color: #CC0000">pytorch</a> neural network framework. Code is available in this github repository:
<a href="" style="color: #CC0000"></a>. <b>Coming soon</b>!

<!-- [comment]: Video
<h3> Video </h3>
<center>
<iframe width="900" height="500" src="https://www.youtube.com/embed/OeBhDK8mQa8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center> -->