---
title: "Future of Egocentric Vision"
layout: gridlay
excerpt: "Future of Egocentric Vision"
sitemap: false
permalink: /egofuture/
---

[comment]: Title
<h1 align="center">An Outlook into the Future of Egocentric Vision</h1>

[comment]: Authors
<p style="text-align: center;">
<a href="https://chiaraplizz.github.io/" style="font-size:24px"> Chiara Plizzari*</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://ezius07.github.io/" style="font-size:24px"> Gabriele Goletto*</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://www.antoninofurnari.it/" style="font-size:24px"> Antonino Furnari*</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://sid2697.github.io" style="font-size:24px"> Siddhant Bansal*</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://iplab.dmi.unict.it/ragusa/" style="font-size:24px"> Francesco Ragusa*</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://www.dmi.unict.it/farinella/" style="font-size:24px"> Giovanni Maria Farinella</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="https://dimadamen.github.io/" style="font-size:24px"> Dima Damen</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="http://www.tatianatommasi.com/" style="font-size:24px"> Tatiana Tommasi</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</p>

<p style="text-align: center;">
<a href="https://arxiv.org/abs/2308.07123" style="font-size:32px"> Paper</a>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
</p>

<center>
<figure>
    <div id="projectid">
    <img src="{{ site.url }}{{ site.baseurl }}/images/projectpic/survey_paper_diagram_collage.png" width="900px" />
    </div>
    <p>&nbsp;</p>
    <figcaption style="text-align: justify; color:Black; font-size:17px">
    We envision a wearable device, <i>EgoAI</i>, that enables in-situ multimodal sensing from the wearer's perspective and provides ego-based assistance. The envisaged future takes the shape of five distinctive use cases that are grounded in either a location or occupation. For example, Ego-Designer, Ego-Worker, and Ego-Tourist shown here.
    </figcaption>
</figure>
</center>

<hr style="border-width: 2px;">

[comment]: EgoFuture
<h1 align="center"><span>Abstract</span></h1>
<br>
<div style="text-align: justify; color:Black; font-size:18px">
What will the future be? We wonder! In this survey, we explore the gap between current research in egocentric vision and the ever-anticipated future, where wearable computing, with outward facing cameras and digital overlays, is expected to be integrated in our every day lives. To understand this gap, the article starts by envisaging the future through character-based stories, showcasing through examples the limitations of current technology. We then provide a mapping between this future and previously defined research tasks. For each task, we survey its seminal works, current state-of-the-art methodologies and available datasets, then reflect on shortcomings that limit its applicability to future research. Note that this survey focuses on software models for egocentric vision, independent of any specific hardware. The paper concludes with recommendations for areas of immediate explorations so as to unlock our path to the future always-on, personalised and life-enhancing egocentric vision.
</div>

<hr style="border-width: 2px;">

[comment]: FutureImagine 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<h1 align="center"><span>Imagining the Future</span></h1>

<p>&nbsp;</p>
<center>
<figure>
		<div id="projectid">
    <img src="{{ site.url }}{{ site.baseurl }}/images/projectpic/two_stories.png" width="900px" />
		</div>
    <p>&nbsp;</p>
    <figcaption style="text-align: justify; color:Black; font-size:17px">
    Here we show character story for two scenarios Ego-Home and Ego-Police. These stories envision the future of egocentric vision. For example, EgoAI helps Sam prepare dinner and keeps him entertained with interactive and immersive experiences. In another scenario, EgoAI assists Judy during her day to keep the city safe.
    </figcaption>
</figure>
</center>

<hr style="border-width: 2px;">
[comment]: LinkStories 
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<h1 align="center"><span>From Narratives to Research Tasks</span></h1>

<p>&nbsp;</p>
<center>
<figure>
		<div id="projectid">
    <img src="{{ site.url }}{{ site.baseurl }}/images/projectpic/stories_link.png" width="900px" />
		</div>
    <p>&nbsp;</p>
    <figcaption style="text-align: justify; color:Black; font-size:17px">
    We connect the narratives in our stories to research tasks in egocentric vision. For each of the use case, we show the corresponding research tasks, along with the specific part of the story where the tasks are occurring.
    </figcaption>
</figure>
</center>
<p>&nbsp;</p>

<hr style="border-width: 2px;">
[comment]: TasksExplored
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<h1 align="center"><span>Research Tasks and Capabilities</span></h1>

<p>&nbsp;</p>

<div style="text-align: justify; color:Black; font-size:20px">
We explore in detail various egocentric vision tasks. We focus on <i>seminal works</i>, <i>state-of-the-art methods</i>, <i>datasets</i>, and <i>for the future</i> for the following tasks:
</div>
<br>
<div align="center" style="font-size: 20px;">

| Localisation ||| 3D Scene Understanding |
| Recognition        ||| Anticipation |
| Gaze Understanding and Prediction        ||| Social Behaviour Understanding |
| Full-body Pose Estimation ||| Hand and Hand-Object Interactions |
| Person Identification ||| Summarisation |
| Dialogue ||| Privacy |

</div>

<hr style="border-width: 2px;">

Please consider citing if you make use of the work:

```
@article{plizzari2024outlook,
      title={An Outlook into the Future of Egocentric Vision}, 
      author={Chiara Plizzari and Gabriele Goletto and Antonino Furnari and Siddhant Bansal and Francesco Ragusa and Giovanni Maria Farinella and Dima Damen and Tatiana Tommasi},
      year={2024},
      journal={International Journal of Computer Vision},
}
```

<p>&nbsp;</p>
<p>&nbsp;</p>
